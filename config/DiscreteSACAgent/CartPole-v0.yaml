env_wrapper: no_wrapper
trainer: offline
trainer_params:
  env_steps: 50000
  batch_size: 32
  burn_in: 500
  train_steps_per_env_step: 1
  render: False
  train_every: 4
agent: discrete_sac
agent_params:
  gamma: 0.99
  lr_pi: 0.0003
  lr_q: 0.0003
  lr_a: 0.0003
  max_memory: 50000
  trainable_alpha: True
  reward_scale: 1
  alpha: 6
  polyak_tau: 0.005
  pi_hidden_size: 64
  q_hidden_size: 64
  min_alpha: 0.001